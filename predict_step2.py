"""
ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰

æ”¹å–„ç‚¹:
- ãƒ¬ãƒ¼ã‚¹IDã®æŸ”è»ŸãªæŠ½å‡ºï¼ˆæ§˜ã€…ãªURLå½¢å¼ã«å¯¾å¿œï¼‰
- è¤‡å‹ç¢ºç‡ã®è¨ˆç®—
- ä¸‰é€£è¤‡ï¼ˆãƒˆãƒªã‚ªï¼‰ç¢ºç‡ã®è¿½åŠ 
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®å¼·åŒ–
- Supabaseã‹ã‚‰éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
"""

import numpy as np
import pandas as pd
import joblib
import re
import os
from pathlib import Path

from fetch_race import fetch_race_data
from preprocess_predict import preprocess_for_prediction
from plackett_luce import simulate_plackett_luce

# Supabaseéå»ãƒ¬ãƒ¼ã‚¹DB
try:
    from supabase_horse_history import SupabaseHorseHistoryDB, calculate_recent_features_supabase
    SUPABASE_AVAILABLE = True
except ImportError:
    SUPABASE_AVAILABLE = False
    print("âš ï¸  supabase_horse_history.py ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

MODEL_PATH = "horse_racing_full_model.txt"


def extract_race_id(url_or_id):
    """
    æ§˜ã€…ãªå½¢å¼ã®URLã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’æŠ½å‡º
    
    å¯¾å¿œå½¢å¼:
    - https://race.netkeiba.com/race/shutuba.html?race_id=202406030811
    - https://race.netkeiba.com/race/result.html?race_id=202406030811
    - https://db.netkeiba.com/race/202406030811
    - 202406030811 (ç›´æ¥ID)
    
    Args:
        url_or_id: URLæ–‡å­—åˆ—ã¾ãŸã¯ãƒ¬ãƒ¼ã‚¹ID
    
    Returns:
        ãƒ¬ãƒ¼ã‚¹ID (12æ¡ã®æ•°å­—)
    """
    # ã™ã§ã«ãƒ¬ãƒ¼ã‚¹IDã®å ´åˆ
    if re.match(r'^\d{12}$', str(url_or_id)):
        return str(url_or_id)
    
    # URLã‹ã‚‰ãƒ¬ãƒ¼ã‚¹IDã‚’æŠ½å‡º
    patterns = [
        r'race_id=(\d{12})',           # race_id=ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        r'/race/(\d{12})',              # /race/12345å½¢å¼
        r'/shutuba\.html.*?(\d{12})',   # shutuba.htmlã®å¾Œ
        r'/result\.html.*?(\d{12})',    # result.htmlã®å¾Œ
        r'(\d{12})',                    # 12æ¡ã®æ•°å­—
    ]
    
    for pattern in patterns:
        match = re.search(pattern, str(url_or_id))
        if match:
            return match.group(1)
    
    raise ValueError(
        f"ãƒ¬ãƒ¼ã‚¹IDã‚’æŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ: {url_or_id}\n"
        f"æœ‰åŠ¹ãªå½¢å¼:\n"
        f"  - https://race.netkeiba.com/race/shutuba.html?race_id=202406030811\n"
        f"  - https://db.netkeiba.com/race/202406030811\n"
        f"  - 202406030811 (12æ¡ã®æ•°å­—)"
    )


def softmax(x):
    """
    ã‚¹ã‚³ã‚¢ã‚’ç¢ºç‡ã«å¤‰æ›ï¼ˆSoftmaxé–¢æ•°ï¼‰
    """
    x = np.array(x, dtype=float)
    x = x - np.max(x)  # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–
    exp_x = np.exp(x)
    return exp_x / exp_x.sum()


def calculate_quinella_place(place_probs, horse_ids, top_n=20):
    """
    è¤‡å‹ï¼ˆé¦¬é€£çš„ä¸­ï¼‰ç¢ºç‡ã‚’è¨ˆç®—
    
    Args:
        place_probs: å„é¦¬ã®3ç€ä»¥å†…ç¢ºç‡
        horse_ids: é¦¬ç•ªãƒªã‚¹ãƒˆ
        top_n: ä¸Šä½ä½•çµ„ã‚’è¿”ã™ã‹
    
    Returns:
        DataFrame with columns: horse1, horse2, prob
    """
    results = []
    n = len(horse_ids)
    
    for i in range(n):
        for j in range(i + 1, n):
            # ä¸¡æ–¹ãŒ3ç€ä»¥å†…ã«å…¥ã‚‹ç¢ºç‡ï¼ˆç°¡æ˜“è¨ˆç®—ï¼‰
            prob = place_probs[horse_ids[i]] * place_probs[horse_ids[j]]
            results.append({
                "é¦¬ç•ª1": horse_ids[i],
                "é¦¬ç•ª2": horse_ids[j],
                "ç¢ºç‡": prob
            })
    
    df = pd.DataFrame(results)
    return df.sort_values("ç¢ºç‡", ascending=False).head(top_n).reset_index(drop=True)


def predict_race(url_or_id, model_path=MODEL_PATH, n_sim=30000, use_supabase=True):
    """
    ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬ã‚’å®Ÿè¡Œ
    
    Args:
        url_or_id: netkeibaã®URLã¾ãŸã¯ãƒ¬ãƒ¼ã‚¹ID
        model_path: ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        n_sim: Plackett-Luceã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å›æ•°
        use_supabase: Supabaseã‹ã‚‰éå»ãƒ¬ãƒ¼ã‚¹ã‚’å–å¾—ã™ã‚‹ã‹
    
    Returns:
        df_race: å„é¦¬ã®äºˆæ¸¬çµæœ
        df_trifecta: ä¸‰é€£å˜TOP10
        df_trio: ä¸‰é€£è¤‡TOP10
        df_quinella_place: è¤‡å‹ï¼ˆé¦¬é€£çš„ä¸­ï¼‰TOP20
    """
    # ===== â‘  ãƒ¬ãƒ¼ã‚¹IDæŠ½å‡º =====
    race_id = extract_race_id(url_or_id)
    print(f"ğŸ‡ ãƒ¬ãƒ¼ã‚¹ID: {race_id}")
    
    # ===== â‘¡ ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿å–å¾— =====
    # URLã‚’å†æ§‹ç¯‰ï¼ˆçµ±ä¸€å½¢å¼ï¼‰
    standard_url = f"https://race.netkeiba.com/race/shutuba.html?race_id={race_id}"
    
    print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {standard_url}")
    df_race = fetch_race_data(standard_url)
    
    if df_race.empty:
        raise ValueError("å‡ºèµ°é¦¬ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
    
    print(f"âœ“ å‡ºèµ°é ­æ•°: {len(df_race)}é ­")
    
    # ===== â‘¢ Supabaseã‹ã‚‰éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾— =====
    if use_supabase and SUPABASE_AVAILABLE:
        try:
            print("ğŸ“š Supabaseã‹ã‚‰éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­...")
            
            # Supabaseæ¥ç¶šï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
            supabase_url = os.getenv("SUPABASE_URL")
            supabase_key = os.getenv("SUPABASE_KEY")
            
            if supabase_url and supabase_key:
                supabase_db = SupabaseHorseHistoryDB(url=supabase_url, key=supabase_key)
                
                # ãƒ¬ãƒ¼ã‚¹æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒªãƒ¼ã‚¯é˜²æ­¢ï¼‰
                race_date = None
                if 'race_date' in df_race.columns:
                    race_date = df_race['race_date'].iloc[0]
                
                # éå»ãƒ¬ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’è¿½åŠ 
                df_race = calculate_recent_features_supabase(
                    df_race, 
                    supabase_db, 
                    n_races=3
                )
                print("âœ“ éå»ãƒ¬ãƒ¼ã‚¹ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã¾ã—ãŸ")
            else:
                print("âš ï¸  ç’°å¢ƒå¤‰æ•° SUPABASE_URL ã¨ SUPABASE_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                print("   éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãªã—ã§ç¶šè¡Œã—ã¾ã™")
        except Exception as e:
            print(f"âš ï¸  Supabaseã‹ã‚‰ã®å–å¾—ã«å¤±æ•—: {e}")
            print("   éå»ãƒ¬ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ãªã—ã§ç¶šè¡Œã—ã¾ã™")
    
    # ===== â‘£ å‰å‡¦ç† =====
    print("ğŸ”§ ç‰¹å¾´é‡ã‚’ç”Ÿæˆä¸­...")
    X = preprocess_for_prediction(df_race)
    
    # ===== â‘¤ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ =====
    if not Path(model_path).exists():
        raise FileNotFoundError(f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {model_path}")
    
    print(f"ğŸ¤– ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰: {model_path}")
    model = joblib.load(model_path)
    
    # ===== â‘¥ Rankerã‚¹ã‚³ã‚¢äºˆæ¸¬ =====
    print("ğŸ¯ äºˆæ¸¬ã‚’å®Ÿè¡Œä¸­...")
    scores = model.predict(X)
    
    # ===== â‘¦ ã‚¹ã‚³ã‚¢ â†’ å‹ç‡å¤‰æ› =====
    df_race["win_prob"] = softmax(scores)
    
    # ===== â‘§ Plackettâ€“Luce ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ =====
    print(f"ğŸ² {n_sim:,}å›ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ä¸­...")
    horse_ids = df_race["horse_no"].tolist()
    win_probs = df_race["win_prob"].values
    
    win_sim, place_prob, trifecta_prob, trio_prob = simulate_plackett_luce(
        horse_ids=horse_ids,
        win_probs=win_probs,
        n_sim=n_sim
    )
    
    df_race["win_prob_sim"] = df_race["horse_no"].map(win_sim)
    df_race["place_prob"] = df_race["horse_no"].map(place_prob)
    
    # ===== â‘¨ ä¸‰é€£å˜ TOP10 =====
    df_trifecta = (
        pd.DataFrame([
            {"1ç€": k[0], "2ç€": k[1], "3ç€": k[2], "ç¢ºç‡": v}
            for k, v in trifecta_prob.items()
        ])
        .sort_values("ç¢ºç‡", ascending=False)
        .head(10)
        .reset_index(drop=True)
    )
    df_trifecta["ç¢ºç‡"] = df_trifecta["ç¢ºç‡"] * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
    
    # ===== â‘© ä¸‰é€£è¤‡ TOP10 =====
    df_trio = (
        pd.DataFrame([
            {"é¦¬ç•ª1": k[0], "é¦¬ç•ª2": k[1], "é¦¬ç•ª3": k[2], "ç¢ºç‡": v}
            for k, v in trio_prob.items()
        ])
        .sort_values("ç¢ºç‡", ascending=False)
        .head(10)
        .reset_index(drop=True)
    )
    df_trio["ç¢ºç‡"] = df_trio["ç¢ºç‡"] * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
    
    # ===== â‘ª è¤‡å‹ï¼ˆé¦¬é€£çš„ä¸­ï¼‰TOP20 =====
    df_quinella_place = calculate_quinella_place(place_prob, horse_ids, top_n=20)
    df_quinella_place["ç¢ºç‡"] = df_quinella_place["ç¢ºç‡"] * 100  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
    
    print("âœ… äºˆæ¸¬å®Œäº†ï¼")
    
    return df_race, df_trifecta, df_trio, df_quinella_place


def display_predictions(df_race, df_trifecta, df_trio, df_quinella_place):
    """
    äºˆæ¸¬çµæœã‚’è¦‹ã‚„ã™ãè¡¨ç¤º
    """
    print("\n" + "="*80)
    print("ğŸ‡ å˜å‹ãƒ»è¤‡å‹äºˆæ¸¬")
    print("="*80)
    
    display_df = df_race[[
        "horse_no",
        "horse_name",
        "win_prob_sim",
        "place_prob"
    ]].copy()
    
    display_df.columns = ["é¦¬ç•ª", "é¦¬å", "å˜å‹ç¢ºç‡", "è¤‡å‹ç¢ºç‡"]
    display_df["å˜å‹ç¢ºç‡"] = (display_df["å˜å‹ç¢ºç‡"] * 100).round(2)
    display_df["è¤‡å‹ç¢ºç‡"] = (display_df["è¤‡å‹ç¢ºç‡"] * 100).round(2)
    
    print(display_df.sort_values("å˜å‹ç¢ºç‡", ascending=False).to_string(index=False))
    
    print("\n" + "="*80)
    print("ğŸ¯ ä¸‰é€£å˜ TOP10")
    print("="*80)
    print(df_trifecta.to_string(index=False))
    
    print("\n" + "="*80)
    print("ğŸ² ä¸‰é€£è¤‡ TOP10")
    print("="*80)
    print(df_trio.to_string(index=False))
    
    print("\n" + "="*80)
    print("ğŸ’° è¤‡å‹ç‹™ã„ï¼ˆé¦¬é€£çš„ä¸­ï¼‰TOP20")
    print("="*80)
    print(df_quinella_place.head(10).to_string(index=False))


if __name__ == "__main__":
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    import sys
    
    if len(sys.argv) > 1:
        url_or_id = sys.argv[1]
    else:
        print("="*80)
        print("ğŸ‡ ç«¶é¦¬ãƒ¬ãƒ¼ã‚¹äºˆæ¸¬")
        print("="*80)
        print("\nURLã¾ãŸã¯ãƒ¬ãƒ¼ã‚¹IDã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
        print("ä¾‹:")
        print("  - https://race.netkeiba.com/race/shutuba.html?race_id=202406030811")
        print("  - https://db.netkeiba.com/race/202406030811")
        print("  - 202406030811")
        print()
        url_or_id = input("å…¥åŠ›: ").strip()
    
    try:
        df_race, df_trifecta, df_trio, df_quinella = predict_race(url_or_id)
        display_predictions(df_race, df_trifecta, df_trio, df_quinella)
        
    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
