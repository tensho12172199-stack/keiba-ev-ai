"""
ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼‰

training_config.yaml ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€
ç‰¹å¾´é‡ã®é‡ã¿ä»˜ã‘ã‚„å®Ÿé¨“ç®¡ç†ã‚’ç°¡å˜ã«è¡Œãˆã¾ã™ã€‚

ä½¿ç”¨æ–¹æ³•:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å®Ÿè¡Œ
    python train_lgbm_ranker_config.py
    
    # å®Ÿé¨“ã‚’æŒ‡å®šã—ã¦å®Ÿè¡Œ
    python train_lgbm_ranker_config.py --experiment weak_recent
    
    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
    python train_lgbm_ranker_config.py --config my_config.yaml
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import joblib
from pathlib import Path
import gc
import glob
import warnings
from datetime import datetime
from typing import List, Tuple
import argparse

# è‡ªä½œãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from feature_engineering import apply_all_features
from add_passing_features import add_passing_features
from add_jockey_style_features import add_jockey_style_features
from add_speed_features import add_speed_features
from add_distance_preference_features import add_distance_preference_features
from add_recent_diff_features import add_recent_diff_features
from config_utils import TrainingConfig
from feature_metadata import FeatureMetadata, extract_feature_metadata_from_training

warnings.filterwarnings('ignore')


def load_csv_files(data_dir: Path) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿"""
    print("\n" + "="*60)
    print("ğŸ“‚ CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™...")
    
    csv_files = sorted(data_dir.glob("*.csv"))
    
    if not csv_files:
        csv_files = sorted(glob.glob("*.csv"))
        if not csv_files:
            raise FileNotFoundError(
                f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
                f"   {data_dir} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
            )
    
    print(f"   è¦‹ã¤ã‹ã£ãŸãƒ•ã‚¡ã‚¤ãƒ«: {len(csv_files)}å€‹")
    for f in csv_files:
        name = f.name if hasattr(f, 'name') else Path(f).name
        print(f"   - {name}")
    
    dfs = []
    for f in csv_files:
        try:
            df = pd.read_csv(f)
            dfs.append(df)
        except Exception as e:
            name = f.name if hasattr(f, 'name') else Path(f).name
            print(f"âš ï¸  {name} ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
    
    if not dfs:
        raise ValueError("æœ‰åŠ¹ãªCSVãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
    
    df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,} è¡Œ")
    
    return df


def preprocess_basic(df: pd.DataFrame, target: str, date_key: str) -> pd.DataFrame:
    """åŸºæœ¬çš„ãªå‰å‡¦ç†"""
    print("\n" + "="*60)
    print("ğŸ”§ åŸºæœ¬å‰å‡¦ç†ã‚’å®Ÿè¡Œä¸­...")
    
    initial_rows = len(df)
    
    # é †ä½ã®æ•°å€¤åŒ–ã¨æ¬ æé™¤å»
    df[target] = pd.to_numeric(df[target], errors="coerce")
    df = df.dropna(subset=[target])
    df[target] = df[target].astype(int)
    
    removed_rows = initial_rows - len(df)
    if removed_rows > 0:
        print(f"   ç„¡åŠ¹ãªé †ä½ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å»: {removed_rows:,} è¡Œ")
    
    # æ—¥ä»˜å‹å¤‰æ›
    if date_key in df.columns:
        df[date_key] = pd.to_datetime(df[date_key], errors="coerce")
        date_nulls = df[date_key].isna().sum()
        if date_nulls > 0:
            print(f"   âš ï¸  æ—¥ä»˜ãŒç„¡åŠ¹: {date_nulls:,} è¡Œ")
    
    # å…±é€šã®å‰å‡¦ç†
    df = apply_all_features(df)
    
    print(f"âœ… åŸºæœ¬å‰å‡¦ç†å®Œäº†: {len(df):,} è¡Œ")
    
    return df


def generate_features(df: pd.DataFrame, config: TrainingConfig) -> pd.DataFrame:
    """é«˜åº¦ãªç‰¹å¾´é‡ç”Ÿæˆ"""
    print("\n" + "="*60)
    print("ğŸ¯ ç‰¹å¾´é‡ç”Ÿæˆã‚’å®Ÿè¡Œä¸­...")
    
    data_config = config.get_data_config()
    horse_key = data_config['horse_key']
    date_key = data_config['date_key']
    race_id = data_config['race_id']
    
    # horse_id ãŒãªã„å ´åˆã¯ç”Ÿæˆ
    if "horse_id" not in df.columns and horse_key in df.columns:
        print(f"   horse_id ã‚’ {horse_key} ã‹ã‚‰ç”Ÿæˆ")
        df["horse_id"] = pd.factorize(df[horse_key])[0]
    
    # ã‚½ãƒ¼ãƒˆ
    if date_key in df.columns:
        df = df.sort_values([date_key, race_id])
    
    # å„ç‰¹å¾´é‡ã®è¿½åŠ 
    n_recent = config.config['features']['n_recent']
    
    feature_funcs = [
        ("é€šéé †ç‰¹å¾´é‡", add_passing_features, ["passing"]),
        ("ã‚¹ãƒ”ãƒ¼ãƒ‰ç‰¹å¾´é‡", add_speed_features, ["distance", "time_sec"]),
        ("è·é›¢é©æ€§ç‰¹å¾´é‡", add_distance_preference_features, ["distance", "speed"]),
        ("é¨æ‰‹å‚¾å‘ç‰¹å¾´é‡", lambda x: add_jockey_style_features(x, config.get_paths()['jockey_profile']), ["jockey"]),
        ("è¿‘èµ°å·®åˆ†ç‰¹å¾´é‡", lambda x: add_recent_diff_features(x, n_recent=n_recent), []),
    ]
    
    for name, func, required_cols in feature_funcs:
        try:
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                print(f"   âš ï¸  {name}: å¿…é ˆã‚«ãƒ©ãƒ ãŒä¸è¶³ {missing_cols} - ã‚¹ã‚­ãƒƒãƒ—")
                continue
            
            print(f"   âœ“ {name} ã‚’è¿½åŠ ä¸­...")
            df = func(df)
        except Exception as e:
            print(f"   âš ï¸  {name} ã®ç”Ÿæˆã«å¤±æ•—: {e}")
    
    print(f"âœ… ç‰¹å¾´é‡ç”Ÿæˆå®Œäº†")
    
    return df


def prepare_dataset(
    df: pd.DataFrame,
    config: TrainingConfig
) -> Tuple[List[str], pd.DataFrame, pd.DataFrame]:
    """å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™"""
    print("\n" + "="*60)
    print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ§‹ç¯‰ä¸­...")
    
    data_config = config.get_data_config()
    date_key = data_config['date_key']
    race_id = data_config['race_id']
    train_end_date = data_config['train_end_date']
    
    # æ•°å€¤å‹ã®ã¿æŠ½å‡º
    numeric_cols = df.select_dtypes(include=["int64", "float64"]).columns.tolist()
    
    # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«åŸºã¥ã„ã¦ç‰¹å¾´é‡ã‚’ãƒ•ã‚£ãƒ«ã‚¿
    features = config.filter_features(numeric_cols)
    
    print(f"   ç‰¹å¾´é‡æ•°: {len(features)}")
    
    # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
    if date_key in df.columns:
        train_end = pd.to_datetime(train_end_date)
        train_df = df[df[date_key] <= train_end].copy()
        valid_df = df[df[date_key] > train_end].copy()
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,} è¡Œ (ï½{train_end_date})")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(valid_df):,} è¡Œ ({train_end_date}ï½)")
    else:
        print("   âš ï¸  æ—¥ä»˜ã‚«ãƒ©ãƒ ãŒãªã„ãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ ã«8:2åˆ†å‰²")
        train_df = df.sample(frac=0.8, random_state=42)
        valid_df = df.drop(train_df.index)
        
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_df):,} è¡Œ")
        print(f"   æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: {len(valid_df):,} è¡Œ")
    
    # ãƒ¬ãƒ¼ã‚¹IDã§ã‚½ãƒ¼ãƒˆ
    train_df = train_df.sort_values(race_id)
    valid_df = valid_df.sort_values(race_id)
    
    return features, train_df, valid_df


def create_ranker_dataset(
    df: pd.DataFrame,
    features: List[str],
    target: str,
    race_id: str,
    config: TrainingConfig
) -> Tuple[pd.DataFrame, np.ndarray, np.ndarray]:
    """LightGBM Rankerç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆé‡ã¿ä»˜ãï¼‰"""
    X = df[features].copy()
    
    # ç‰¹å¾´é‡ã®é‡ã¿ä»˜ã‘ã‚’é©ç”¨
    X = config.apply_feature_weights(X)
    
    y = df[target]
    group = df.groupby(race_id).size().to_numpy()
    
    return X, y, group


def train_model(
    train_data: Tuple[pd.DataFrame, np.ndarray, np.ndarray],
    valid_data: Tuple[pd.DataFrame, np.ndarray, np.ndarray],
    features: List[str],
    config: TrainingConfig
) -> lgb.LGBMRanker:
    """ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’"""
    print("\n" + "="*60)
    print("ğŸš€ ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’é–‹å§‹...")
    
    X_train, y_train, group_train = train_data
    X_valid, y_valid, group_valid = valid_data
    
    print(f"   è¨“ç·´ãƒ¬ãƒ¼ã‚¹æ•°: {len(group_train):,}")
    print(f"   æ¤œè¨¼ãƒ¬ãƒ¼ã‚¹æ•°: {len(group_valid):,}")
    
    # LightGBMãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—
    lgbm_params = config.get_lgbm_params()
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    model = lgb.LGBMRanker(**lgbm_params)
    
    # å­¦ç¿’
    start_time = datetime.now()
    
    eval_period = config.config['logging']['log_evaluation_period']
    early_stopping = lgbm_params.get('early_stopping_rounds', 50)
    
    model.fit(
        X_train, y_train,
        group=group_train,
        eval_set=[(X_valid, y_valid)],
        eval_group=[group_valid],
        eval_metric="ndcg",
        callbacks=[
            lgb.log_evaluation(period=eval_period),
            lgb.early_stopping(stopping_rounds=early_stopping, verbose=True)
        ]
    )
    
    elapsed = (datetime.now() - start_time).total_seconds()
    print(f"\nâœ… å­¦ç¿’å®Œäº† (æ‰€è¦æ™‚é–“: {elapsed:.1f}ç§’)")
    
    return model


def evaluate_model(
    model: lgb.LGBMRanker,
    valid_data: Tuple[pd.DataFrame, np.ndarray, np.ndarray]
) -> pd.DataFrame:
    """ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡"""
    print("\n" + "="*60)
    print("ğŸ“ˆ ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ä¸­...")
    
    X_valid, y_valid, group_valid = valid_data
    
    y_pred = model.predict(X_valid)
    
    metrics = []
    start_idx = 0
    
    for group_size in group_valid:
        end_idx = start_idx + group_size
        
        race_true = y_valid.iloc[start_idx:end_idx].values
        race_pred = y_pred[start_idx:end_idx]
        
        top3_pred_idx = np.argsort(race_pred)[:3]
        top3_true = race_true[top3_pred_idx]
        
        hit_top3 = np.sum(top3_true <= 3)
        
        metrics.append({
            "race_size": group_size,
            "hit_top3": hit_top3
        })
        
        start_idx = end_idx
    
    metrics_df = pd.DataFrame(metrics)
    
    accuracy_top3 = (metrics_df["hit_top3"] > 0).mean()
    avg_hit = metrics_df["hit_top3"].mean()
    
    print(f"   æ¤œè¨¼ãƒ¬ãƒ¼ã‚¹æ•°: {len(metrics_df):,}")
    print(f"   Top3çš„ä¸­ç‡: {accuracy_top3:.2%}")
    print(f"   å¹³å‡çš„ä¸­é ­æ•°: {avg_hit:.2f}")
    
    return metrics_df


def save_artifacts(
    model: lgb.LGBMRanker,
    features: List[str],
    metrics: pd.DataFrame,
    config: TrainingConfig,
    train_df: pd.DataFrame = None
) -> None:
    """ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜"""
    print("\n" + "="*60)
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã¨çµæœã‚’ä¿å­˜ä¸­...")
    
    paths = config.get_paths()
    out_dir = Path(paths['output_dir'])
    out_dir.mkdir(exist_ok=True, parents=True)
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    joblib.dump(model, paths['model_file'])
    print(f"   âœ“ ãƒ¢ãƒ‡ãƒ«: {paths['model_file']}")
    
    # ç‰¹å¾´é‡ãƒªã‚¹ãƒˆä¿å­˜
    joblib.dump(features, paths['feature_list_file'])
    print(f"   âœ“ ç‰¹å¾´é‡ãƒªã‚¹ãƒˆ: {paths['feature_list_file']}")
    
    # ç‰¹å¾´é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆNEW!ï¼‰
    if train_df is not None:
        try:
            metadata = extract_feature_metadata_from_training(train_df, features, config)
            metadata.save("feature_metadata.json")
            print(f"   âœ“ ç‰¹å¾´é‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿: feature_metadata.json")
        except Exception as e:
            print(f"   âš ï¸  ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ç‰¹å¾´é‡é‡è¦åº¦
    importance_df = pd.DataFrame({
        "feature": features,
        "importance": model.feature_importances_
    }).sort_values("importance", ascending=False)
    
    importance_file = out_dir / "feature_importance.csv"
    importance_df.to_csv(importance_file, index=False)
    print(f"   âœ“ ç‰¹å¾´é‡é‡è¦åº¦: {importance_file}")
    
    # è©•ä¾¡æŒ‡æ¨™
    metrics_file = out_dir / "training_metrics.csv"
    metrics.to_csv(metrics_file, index=False)
    print(f"   âœ“ è©•ä¾¡æŒ‡æ¨™: {metrics_file}")
    
    # é‡è¦åº¦è¡¨ç¤º
    show_n = config.config['logging']['show_feature_importance']
    print(f"\nğŸ“Š ç‰¹å¾´é‡é‡è¦åº¦ (Top {show_n})")
    print("-" * 60)
    for idx, row in importance_df.head(show_n).iterrows():
        print(f"   {row['feature']:40s} {row['importance']:>10.1f}")


def main(args):
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("\n" + "="*80)
    print("ğŸ‡ ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œç‰ˆï¼‰")
    print("="*80)
    
    try:
        # è¨­å®šèª­ã¿è¾¼ã¿
        config = TrainingConfig(args.config)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªå®Ÿé¨“ã‚’åˆ‡ã‚Šæ›¿ãˆ
        if args.experiment:
            config.config['active_experiment'] = args.experiment
            config._apply_experiment()
        
        config.print_summary()
        
        data_config = config.get_data_config()
        paths = config.get_paths()
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        df = load_csv_files(Path(paths['data_dir']))
        
        # å‰å‡¦ç†
        df = preprocess_basic(df, data_config['target'], data_config['date_key'])
        
        # ç‰¹å¾´é‡ç”Ÿæˆ
        df = generate_features(df, config)
        
        gc.collect()
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™
        features, train_df, valid_df = prepare_dataset(df, config)
        
        # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ã‚³ãƒ”ãƒ¼ã‚’ä¿æŒï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç”¨ï¼‰
        train_df_for_metadata = train_df[features].copy()
        
        # Rankerç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆï¼ˆé‡ã¿ä»˜ãï¼‰
        train_data = create_ranker_dataset(
            train_df, features, data_config['target'], data_config['race_id'], config
        )
        valid_data = create_ranker_dataset(
            valid_df, features, data_config['target'], data_config['race_id'], config
        )
        
        del df, train_df, valid_df
        gc.collect()
        
        # å­¦ç¿’
        model = train_model(train_data, valid_data, features, config)
        
        # è©•ä¾¡
        metrics = evaluate_model(model, valid_data)
        
        # ä¿å­˜ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä»˜ãï¼‰
        save_artifacts(model, features, metrics, config, train_df_for_metadata)
        
        print("\n" + "="*80)
        print("âœ… ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print("="*80)
        
    except Exception as e:
        print("\n" + "="*80)
        print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        print("="*80)
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ç«¶é¦¬äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰")
    parser.add_argument(
        '--config',
        type=str,
        default='training_config.yaml',
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: training_config.yamlï¼‰'
    )
    parser.add_argument(
        '--experiment',
        type=str,
        default=None,
        help='å®Ÿé¨“åã‚’æŒ‡å®šï¼ˆä¾‹: weak_recent, balancedï¼‰'
    )
    
    args = parser.parse_args()
    main(args)
